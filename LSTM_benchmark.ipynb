{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gbn8SzZnHKy1",
        "outputId": "0c0fb609-5479-432b-a672-9804c1780916",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Retrieving folder contents\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "Processing file 1J2eOhACVhee6fnrO0wA5ct255sVR5EQS BCNET_regular.csv\n",
            "Processing file 1d4e4g7PNVkxO-H4ZknemlWZNnWOpJkLV Code_Red_I.csv\n",
            "Processing file 1IleVfZkR-EQ0X6-TmM1UXZhDphFn-auG Moscow_blackout.csv\n",
            "Processing file 1YFeW7-KnN474mkULMOAs3h3cxFS7tdb4 Nimda.csv\n",
            "Processing file 1_PlE4ABTghvpYQ7M1MfT8w5xQmXopQok RIPE_regular.csv\n",
            "Processing file 1WjlwJW4dHABkkx0ANAfAQBLIo9OZf5C1 Slammer.csv\n",
            "Processing file 1RKRYfuaEmRCsgUaHIZICxL9FngrMYP7u WannaCrypt.csv\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'doc-10-9o-docs.googleusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1J2eOhACVhee6fnrO0wA5ct255sVR5EQS\n",
            "To: /content/BGP_RIPE_datasets/BCNET_regular.csv\n",
            "100% 151k/151k [00:00<00:00, 64.4MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'doc-0c-9o-docs.googleusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1d4e4g7PNVkxO-H4ZknemlWZNnWOpJkLV\n",
            "To: /content/BGP_RIPE_datasets/Code_Red_I.csv\n",
            "100% 734k/734k [00:00<00:00, 133MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'doc-0c-9o-docs.googleusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IleVfZkR-EQ0X6-TmM1UXZhDphFn-auG\n",
            "To: /content/BGP_RIPE_datasets/Moscow_blackout.csv\n",
            "100% 750k/750k [00:00<00:00, 125MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'doc-10-9o-docs.googleusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YFeW7-KnN474mkULMOAs3h3cxFS7tdb4\n",
            "To: /content/BGP_RIPE_datasets/Nimda.csv\n",
            "100% 885k/885k [00:00<00:00, 108MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'doc-0o-9o-docs.googleusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_PlE4ABTghvpYQ7M1MfT8w5xQmXopQok\n",
            "To: /content/BGP_RIPE_datasets/RIPE_regular.csv\n",
            "100% 773k/773k [00:00<00:00, 103MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'doc-0o-9o-docs.googleusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WjlwJW4dHABkkx0ANAfAQBLIo9OZf5C1\n",
            "To: /content/BGP_RIPE_datasets/Slammer.csv\n",
            "100% 731k/731k [00:00<00:00, 149MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'doc-04-9o-docs.googleusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RKRYfuaEmRCsgUaHIZICxL9FngrMYP7u\n",
            "To: /content/BGP_RIPE_datasets/WannaCrypt.csv\n",
            "100% 1.29M/1.29M [00:00<00:00, 105MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "!gdown --no-check-certificate --folder https://drive.google.com/drive/folders/17g55PHmMWFo6aBNhmzjOxVDfwtSDFUl3?usp=drive_link"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Constants\n",
        "SEQUENCE_LENGTH = 10\n",
        "FEATURES_START = 4\n",
        "FEATURES_END = 41\n",
        "LABEL_COLUMN = 41\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Custom dataset for LSTM\n",
        "class BGPDataset(Dataset):\n",
        "    def __init__(self, data, labels, sequence_length=SEQUENCE_LENGTH):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.sequence_length + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx + self.sequence_length]\n",
        "        y = self.labels[idx + self.sequence_length - 1]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_data(file_paths):\n",
        "    all_data = []\n",
        "    for file_path in file_paths:\n",
        "        df = pd.read_csv(file_path, header=None)\n",
        "        features = df.iloc[:, FEATURES_START:FEATURES_END + 1].values\n",
        "        labels = df.iloc[:, LABEL_COLUMN].replace(-1, 0).values\n",
        "        all_data.append((features, labels))\n",
        "    return all_data\n",
        "\n",
        "# Normalize features\n",
        "def normalize_data(train_features, test_features):\n",
        "    scaler = MinMaxScaler()\n",
        "    train_features = scaler.fit_transform(train_features)\n",
        "    test_features = scaler.transform(test_features)\n",
        "    return train_features, test_features\n",
        "\n",
        "# Compute class weights for imbalanced data\n",
        "def compute_weights(labels):\n",
        "    classes = np.unique(labels)\n",
        "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
        "    return {i: weight for i, weight in zip(classes, class_weights)}\n",
        "\n",
        "# LSTM Model with Bidirectional LSTM and Dropout\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=0.3\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)  # *2 for bidirectional\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out[:, -1, :])  # Use the last timestep's output\n",
        "        return torch.sigmoid(out)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=EPOCHS):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x_batch)\n",
        "            loss = criterion(outputs.squeeze(), y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        scheduler.step(total_loss / len(train_loader))\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Evaluation function with zero_division parameter\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in test_loader:\n",
        "            outputs = model(x_batch)\n",
        "            preds = (outputs.squeeze() > 0.5).int()\n",
        "            all_preds.extend(preds.tolist())\n",
        "            all_labels.extend(y_batch.tolist())\n",
        "    print(classification_report(all_labels, all_preds, digits=4, zero_division=1))\n",
        "\n",
        "# Main script\n",
        "def main(folder_path):\n",
        "    # Get all CSV file paths in the folder\n",
        "    file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
        "\n",
        "    # Split files into training (BCNET and RIPE) and testing (others)\n",
        "    train_files = [f for f in file_paths if 'BCNET_regular' in f or 'RIPE_regular' in f]\n",
        "    test_files = [f for f in file_paths if f not in train_files]\n",
        "\n",
        "    print(\"Training files:\", train_files)\n",
        "    print(\"Testing files:\", test_files)\n",
        "\n",
        "    # Load and preprocess data\n",
        "    train_data = load_data(train_files)\n",
        "    test_data = load_data(test_files)\n",
        "\n",
        "    # Prepare training and testing datasets\n",
        "    train_features = np.vstack([data[0] for data in train_data])\n",
        "    train_labels = np.concatenate([data[1] for data in train_data])\n",
        "    test_features = np.vstack([data[0] for data in test_data])\n",
        "    test_labels = np.concatenate([data[1] for data in test_data])\n",
        "\n",
        "    # Normalize data\n",
        "    train_features, test_features = normalize_data(train_features, test_features)\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights = compute_weights(train_labels)\n",
        "    print(\"Class weights:\", class_weights)\n",
        "\n",
        "    # Create Datasets and DataLoaders\n",
        "    train_dataset = BGPDataset(train_features, train_labels)\n",
        "    test_dataset = BGPDataset(test_features, test_labels)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Model Initialization\n",
        "    input_size = FEATURES_END - FEATURES_START + 1\n",
        "    hidden_size = 128\n",
        "    num_layers = 3\n",
        "    output_size = 1  # Binary classification\n",
        "    model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
        "\n",
        "    # Train and Evaluate\n",
        "    train_model(model, train_loader, criterion, optimizer, scheduler)\n",
        "    evaluate_model(model, test_loader)\n",
        "\n",
        "# Path to the folder containing the datasets\n",
        "folder_path = \"/content/BGP_RIPE_datasets/\"  # Update with your folder path\n",
        "main(folder_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXin1-GooR6S",
        "outputId": "bb5388fe-18c9-4949-84cc-8e3aeeffcf9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training files: ['/content/BGP_RIPE_datasets/BCNET_regular.csv', '/content/BGP_RIPE_datasets/RIPE_regular.csv']\n",
            "Testing files: ['/content/BGP_RIPE_datasets/Code_Red_I.csv', '/content/BGP_RIPE_datasets/Moscow_blackout.csv', '/content/BGP_RIPE_datasets/Slammer.csv', '/content/BGP_RIPE_datasets/Nimda.csv', '/content/BGP_RIPE_datasets/WannaCrypt.csv']\n",
            "Class weights: {0: 1.0}\n",
            "Epoch 1/20, Loss: 0.0354\n",
            "Epoch 2/20, Loss: 0.0000\n",
            "Epoch 3/20, Loss: 0.0000\n",
            "Epoch 4/20, Loss: 0.0000\n",
            "Epoch 5/20, Loss: 0.0000\n",
            "Epoch 6/20, Loss: 0.0000\n",
            "Epoch 7/20, Loss: 0.0000\n",
            "Epoch 8/20, Loss: 0.0000\n",
            "Epoch 9/20, Loss: 0.0000\n",
            "Epoch 10/20, Loss: 0.0000\n",
            "Epoch 11/20, Loss: 0.0000\n",
            "Epoch 12/20, Loss: 0.0000\n",
            "Epoch 13/20, Loss: 0.0000\n",
            "Epoch 14/20, Loss: 0.0000\n",
            "Epoch 15/20, Loss: 0.0000\n",
            "Epoch 16/20, Loss: 0.0000\n",
            "Epoch 17/20, Loss: 0.0000\n",
            "Epoch 18/20, Loss: 0.0000\n",
            "Epoch 19/20, Loss: 0.0000\n",
            "Epoch 20/20, Loss: 0.0000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7898    1.0000    0.8825     32950\n",
            "         1.0     1.0000    0.0000    0.0000      8770\n",
            "\n",
            "    accuracy                         0.7898     41720\n",
            "   macro avg     0.8949    0.5000    0.4413     41720\n",
            "weighted avg     0.8340    0.7898    0.6970     41720\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}