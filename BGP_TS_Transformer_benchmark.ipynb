{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting beautifulsoup4 (from gdown)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /home/tus4zw/.local/lib/python3.11/site-packages (from gdown) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in /home/tus4zw/.local/lib/python3.11/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/tus4zw/.local/lib/python3.11/site-packages (from gdown) (4.67.1)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages (from requests[socks]->gdown) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, PySocks, beautifulsoup4, gdown\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.12.3 gdown-5.2.0 soupsieve-2.6\n",
      "Retrieving folder contents\n",
      "/scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Processing file 1J2eOhACVhee6fnrO0wA5ct255sVR5EQS BCNET_regular.csv\n",
      "Processing file 1d4e4g7PNVkxO-H4ZknemlWZNnWOpJkLV Code_Red_I.csv\n",
      "Processing file 1IleVfZkR-EQ0X6-TmM1UXZhDphFn-auG Moscow_blackout.csv\n",
      "Processing file 1YFeW7-KnN474mkULMOAs3h3cxFS7tdb4 Nimda.csv\n",
      "Processing file 1_PlE4ABTghvpYQ7M1MfT8w5xQmXopQok RIPE_regular.csv\n",
      "Processing file 1WjlwJW4dHABkkx0ANAfAQBLIo9OZf5C1 Slammer.csv\n",
      "Processing file 1RKRYfuaEmRCsgUaHIZICxL9FngrMYP7u WannaCrypt.csv\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "/scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'doc-10-9o-docs.googleusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1J2eOhACVhee6fnrO0wA5ct255sVR5EQS\n",
      "To: /sfs/gpfs/tardis/home/tus4zw/computer_networks/BGP_RIPE_datasets/BCNET_regular.csv\n",
      "100%|████████████████████████████████████████| 151k/151k [00:00<00:00, 19.4MB/s]\n",
      "/scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'doc-0c-9o-docs.googleusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1d4e4g7PNVkxO-H4ZknemlWZNnWOpJkLV\n",
      "To: /sfs/gpfs/tardis/home/tus4zw/computer_networks/BGP_RIPE_datasets/Code_Red_I.csv\n",
      "100%|████████████████████████████████████████| 734k/734k [00:00<00:00, 15.6MB/s]\n",
      "/scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'doc-0c-9o-docs.googleusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1IleVfZkR-EQ0X6-TmM1UXZhDphFn-auG\n",
      "To: /sfs/gpfs/tardis/home/tus4zw/computer_networks/BGP_RIPE_datasets/Moscow_blackout.csv\n",
      "100%|████████████████████████████████████████| 750k/750k [00:00<00:00, 14.9MB/s]\n",
      "/scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'doc-10-9o-docs.googleusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1YFeW7-KnN474mkULMOAs3h3cxFS7tdb4\n",
      "To: /sfs/gpfs/tardis/home/tus4zw/computer_networks/BGP_RIPE_datasets/Nimda.csv\n",
      "100%|████████████████████████████████████████| 885k/885k [00:00<00:00, 25.3MB/s]\n",
      "/scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'doc-0o-9o-docs.googleusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1_PlE4ABTghvpYQ7M1MfT8w5xQmXopQok\n",
      "To: /sfs/gpfs/tardis/home/tus4zw/computer_networks/BGP_RIPE_datasets/RIPE_regular.csv\n",
      "100%|████████████████████████████████████████| 773k/773k [00:00<00:00, 34.9MB/s]\n",
      "/scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'doc-0o-9o-docs.googleusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1WjlwJW4dHABkkx0ANAfAQBLIo9OZf5C1\n",
      "To: /sfs/gpfs/tardis/home/tus4zw/computer_networks/BGP_RIPE_datasets/Slammer.csv\n",
      "100%|████████████████████████████████████████| 731k/731k [00:00<00:00, 13.9MB/s]\n",
      "/scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'drive.google.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/scratch/tus4zw/conda/.conda/envs/qwen/lib/python3.11/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'doc-04-9o-docs.googleusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1RKRYfuaEmRCsgUaHIZICxL9FngrMYP7u\n",
      "To: /sfs/gpfs/tardis/home/tus4zw/computer_networks/BGP_RIPE_datasets/WannaCrypt.csv\n",
      "100%|██████████████████████████████████████| 1.29M/1.29M [00:00<00:00, 23.2MB/s]\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "!gdown --no-check-certificate --folder https://drive.google.com/drive/folders/17g55PHmMWFo6aBNhmzjOxVDfwtSDFUl3?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from TS_Transformer import TSTransformerEncoderClassiregressor\n",
    "def set_all_seeds(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_all_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training files: ['/home/tus4zw/computer_networks/BGP_RIPE_datasets/Nimda.csv', '/home/tus4zw/computer_networks/BGP_RIPE_datasets/Slammer.csv']\n",
      "Testing files: ['/home/tus4zw/computer_networks/BGP_RIPE_datasets/RIPE_regular.csv', '/home/tus4zw/computer_networks/BGP_RIPE_datasets/WannaCrypt.csv', '/home/tus4zw/computer_networks/BGP_RIPE_datasets/Moscow_blackout.csv', '/home/tus4zw/computer_networks/BGP_RIPE_datasets/Code_Red_I.csv', '/home/tus4zw/computer_networks/BGP_RIPE_datasets/BCNET_regular.csv']\n",
      "Unique labels in training data: [0 1]\n",
      "Unique labels in testing data: [0 1]\n",
      "Class weights: {np.int64(0): np.float64(0.5795512867512281), np.int64(1): np.float64(3.6426267281105993)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/tus4zw/conda/.conda/envs/informer/lib/python3.9/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 10.04267\n",
      "Epoch 2/20, Loss: 7.57379\n",
      "Epoch 3/20, Loss: 7.20850\n",
      "Epoch 4/20, Loss: 7.12644\n",
      "Epoch 5/20, Loss: 7.13281\n",
      "Epoch 6/20, Loss: 7.17530\n",
      "Epoch 7/20, Loss: 7.09308\n",
      "Epoch 8/20, Loss: 7.03200\n",
      "Epoch 9/20, Loss: 7.10150\n",
      "Epoch 10/20, Loss: 7.02978\n",
      "Epoch 11/20, Loss: 7.05310\n",
      "Epoch 12/20, Loss: 7.01297\n",
      "Epoch 13/20, Loss: 7.02428\n",
      "Epoch 14/20, Loss: 7.07429\n",
      "Epoch 15/20, Loss: 6.99578\n",
      "Epoch 16/20, Loss: 7.00583\n",
      "Epoch 17/20, Loss: 7.07511\n",
      "Epoch 18/20, Loss: 6.97625\n",
      "Epoch 19/20, Loss: 6.99209\n",
      "Epoch 20/20, Loss: 7.07184\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9997    0.9989    0.9993     27947\n",
      "         1.0     0.9955    0.9988    0.9971      6600\n",
      "\n",
      "    accuracy                         0.9989     34547\n",
      "   macro avg     0.9976    0.9989    0.9982     34547\n",
      "weighted avg     0.9989    0.9989    0.9989     34547\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "SEQUENCE_LENGTH = 10\n",
    "FEATURES_START = 4\n",
    "FEATURES_END = 41\n",
    "LABEL_COLUMN = 41\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Detect if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Custom dataset\n",
    "class BGPDataset(Dataset):\n",
    "    def __init__(self, data, labels, sequence_length=SEQUENCE_LENGTH):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.sequence_length]\n",
    "        y = self.labels[idx + self.sequence_length - 1]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(file_paths):\n",
    "    all_data = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "        features = df.iloc[:, FEATURES_START:FEATURES_END + 1].values\n",
    "        labels = df.iloc[:, LABEL_COLUMN].replace(-1, 0).values\n",
    "        all_data.append((features, labels))\n",
    "    return all_data\n",
    "\n",
    "# Normalize features\n",
    "def normalize_data(train_features, test_features):\n",
    "    scaler = MinMaxScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "    return train_features, test_features\n",
    "\n",
    "# Compute class weights for imbalanced data\n",
    "def compute_weights(labels):\n",
    "    classes = np.unique(labels)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
    "    return {i: weight for i, weight in zip(classes, class_weights)}\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, max_seq_len, d_model, n_heads, num_layers, dim_feedforward, num_classes, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer = TSTransformerEncoderClassiregressor(\n",
    "            feat_dim=input_size,\n",
    "            max_len=max_seq_len,\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            num_classes=num_classes,\n",
    "            dropout=dropout,\n",
    "            pos_encoding='fixed',\n",
    "            activation='gelu',\n",
    "            norm='BatchNorm'\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # No padding mask used in this dataset; provide a mask of ones\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        padding_mask = torch.ones(batch_size, seq_len, dtype=torch.bool, device=x.device)\n",
    "        return self.transformer(x, padding_mask)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=EPOCHS):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step(total_loss / len(train_loader))\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.5f}\")\n",
    "\n",
    "# Evaluation function with zero_division parameter\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            preds = (outputs.squeeze() > 0.5).int()\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(y_batch.cpu().tolist())\n",
    "    print(classification_report(all_labels, all_preds, digits=4, zero_division=1))\n",
    "\n",
    "# Main script\n",
    "def main(folder_path):\n",
    "    # Get all CSV file paths in the folder\n",
    "    file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "    # Split files into training (BCNET and RIPE) and testing (others)\n",
    "    train_files = [f for f in file_paths if 'Nimda' in f or 'Slammer' in f]\n",
    "    test_files = [f for f in file_paths if f not in train_files]\n",
    "\n",
    "    print(\"Training files:\", train_files)\n",
    "    print(\"Testing files:\", test_files)\n",
    "\n",
    "    # Load and preprocess data\n",
    "    train_data = load_data(train_files)\n",
    "    test_data = load_data(test_files)\n",
    "\n",
    "    # Prepare training and testing datasets\n",
    "    train_features = np.vstack([data[0] for data in train_data])\n",
    "    train_labels = np.concatenate([data[1] for data in train_data])\n",
    "    test_features = np.vstack([data[0] for data in test_data])\n",
    "    test_labels = np.concatenate([data[1] for data in test_data])\n",
    "    print(\"Unique labels in training data:\", np.unique(train_labels))\n",
    "    print(\"Unique labels in testing data:\", np.unique(test_labels))\n",
    "\n",
    "    #raise Exception(\"Stop here\")\n",
    "\n",
    "    # Normalize data\n",
    "    train_features, test_features = normalize_data(train_features, test_features)\n",
    "\n",
    "    # Compute class weights\n",
    "    class_weights = compute_weights(train_labels)\n",
    "    print(\"Class weights:\", class_weights)\n",
    "    class_weights_tensor = torch.tensor([class_weights[i] for i in range(2)], dtype=torch.float32).to(device)\n",
    "    # Create Datasets and DataLoaders\n",
    "    train_dataset = BGPDataset(train_features, train_labels)\n",
    "    test_dataset = BGPDataset(test_features, test_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Model Initialization\n",
    "    input_size = FEATURES_END - FEATURES_START + 1\n",
    "    max_seq_len = SEQUENCE_LENGTH\n",
    "    d_model = 16\n",
    "    n_heads = 2\n",
    "    num_layers = 2\n",
    "    dim_feedforward = 16\n",
    "    num_classes = 1  # Binary classification\n",
    "    dropout = 0.6\n",
    "\n",
    "    model = TransformerModel(input_size, max_seq_len, d_model, n_heads, num_layers, dim_feedforward, num_classes, dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1)\n",
    "\n",
    "    # Train and Evaluate\n",
    "    train_model(model, train_loader, criterion, optimizer, scheduler)\n",
    "    evaluate_model(model, test_loader)\n",
    "\n",
    "# Path to the folder containing the datasets\n",
    "folder_path = \"/home/tus4zw/computer_networks/BGP_RIPE_datasets\"  # Update with your folder path\n",
    "main(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files: ['/home/tus4zw/computer_networks/BGP_RIPE_datasets/Nimda.csv', '/home/tus4zw/computer_networks/BGP_RIPE_datasets/Slammer.csv']\n",
      "Testing files: ['/home/tus4zw/computer_networks/BGP_RIPE_datasets/RIPE_regular.csv', '/home/tus4zw/computer_networks/BGP_RIPE_datasets/WannaCrypt.csv', '/home/tus4zw/computer_networks/BGP_RIPE_datasets/Moscow_blackout.csv', '/home/tus4zw/computer_networks/BGP_RIPE_datasets/Code_Red_I.csv', '/home/tus4zw/computer_networks/BGP_RIPE_datasets/BCNET_regular.csv']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.10312487151090108\n",
      "Epoch 2/20, Loss: 0.007148597427871953\n",
      "Epoch 3/20, Loss: 0.00427214672683167\n",
      "Epoch 4/20, Loss: 0.0038857036808091837\n",
      "Epoch 5/20, Loss: 0.00338605341302049\n",
      "Epoch 6/20, Loss: 0.0020794493033065692\n",
      "Epoch 7/20, Loss: 0.0014950096089378054\n",
      "Epoch 8/20, Loss: 0.0012636703559110174\n",
      "Epoch 9/20, Loss: 0.0008049043365217916\n",
      "Epoch 10/20, Loss: 0.0008794138429377626\n",
      "Epoch 11/20, Loss: 0.00023547446855872874\n",
      "Epoch 12/20, Loss: 0.0010932277314251307\n",
      "Epoch 13/20, Loss: 0.0004405384169292565\n",
      "Epoch 14/20, Loss: 0.00016315606700138477\n",
      "Epoch 15/20, Loss: 8.773484405712504e-05\n",
      "Epoch 16/20, Loss: 5.038360266807424e-05\n",
      "Epoch 17/20, Loss: 4.084580252112006e-05\n",
      "Epoch 18/20, Loss: 0.00012967452521327682\n",
      "Epoch 19/20, Loss: 8.368357237975442e-05\n",
      "Epoch 20/20, Loss: 3.4993820228140726e-05\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9996    0.7887    0.8818     27947\n",
      "         1.0     0.5275    0.9988    0.6904      6600\n",
      "\n",
      "    accuracy                         0.8289     34547\n",
      "   macro avg     0.7636    0.8938    0.7861     34547\n",
      "weighted avg     0.9094    0.8289    0.8452     34547\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "SEQUENCE_LENGTH = 10  # Number of timesteps for LSTM\n",
    "FEATURES_START = 4    # Start index of features\n",
    "FEATURES_END = 41     # End index of features (inclusive)\n",
    "LABEL_COLUMN = 41     # Label column index\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Custom dataset for LSTM\n",
    "class BGPDataset(Dataset):\n",
    "    def __init__(self, data, labels, sequence_length=SEQUENCE_LENGTH):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.sequence_length]\n",
    "        y = self.labels[idx + self.sequence_length - 1]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(file_paths):\n",
    "    all_data = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path, header=None)  # No headers in the file\n",
    "        features = df.iloc[:, FEATURES_START:FEATURES_END + 1].values\n",
    "        labels = df.iloc[:, LABEL_COLUMN].replace(-1, 0).values  # Convert -1 to 0\n",
    "        all_data.append((features, labels))\n",
    "    return all_data\n",
    "\n",
    "# Normalize features on training data only\n",
    "def normalize_data(train_features, test_features):\n",
    "    scaler = MinMaxScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "    test_features = scaler.transform(test_features)\n",
    "    return train_features, test_features\n",
    "\n",
    "# Define LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Use the last time step's output\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "# Training Loop\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=EPOCHS):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            outputs = model(x_batch)\n",
    "            preds = (outputs.squeeze() > 0.5).int()\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(y_batch.tolist())\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n",
    "\n",
    "# Main script\n",
    "def main():\n",
    "\n",
    "    folder_path = \"/home/tus4zw/computer_networks/BGP_RIPE_datasets\"\n",
    "    # Get all CSV file paths in the folder\n",
    "    file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "    # Split files into training (BCNET and RIPE) and testing (others)\n",
    "    train_files = [f for f in file_paths if 'Nimda' in f or 'Slammer' in f]\n",
    "    test_files = [f for f in file_paths if f not in train_files]\n",
    "\n",
    "    print(\"Training files:\", train_files)\n",
    "    print(\"Testing files:\", test_files)\n",
    "\n",
    "    # Load and preprocess data\n",
    "    train_data = load_data(train_files)\n",
    "    test_data = load_data(test_files)\n",
    "\n",
    "    # Prepare training and testing datasets\n",
    "    train_features = np.vstack([data[0] for data in train_data])\n",
    "    train_labels = np.concatenate([data[1] for data in train_data])\n",
    "    test_features = np.vstack([data[0] for data in test_data])\n",
    "    test_labels = np.concatenate([data[1] for data in test_data])\n",
    "\n",
    "    # Normalize data\n",
    "    train_features, test_features = normalize_data(train_features, test_features)\n",
    "\n",
    "    # Create Datasets and DataLoaders\n",
    "    train_dataset = BGPDataset(train_features, train_labels)\n",
    "    test_dataset = BGPDataset(test_features, test_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Model Initialization\n",
    "    input_size = FEATURES_END - FEATURES_START + 1\n",
    "    hidden_size = 64\n",
    "    num_layers = 2\n",
    "    output_size = 1  # Binary classification\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Train and Evaluate\n",
    "    train_model(model, train_loader, criterion, optimizer)\n",
    "    evaluate_model(model, test_loader)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "informer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
